{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for generating audit, model and task jsons\n",
    "\n",
    "### *Use only when the model works perfectly well as 'code.py' script*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize below functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_np_matrix_to_json(a):\n",
    "    output = {}\n",
    "    for i in range(len(a[0])):\n",
    "        output[str(a[0][i])] = int(a[1][i])\n",
    "    return output\n",
    "\n",
    "def summarize_categorical_variable(values, name, model):\n",
    "    model['preprocessing'][name] = {\n",
    "        \"name\": name,\n",
    "        \"type\": \"categorical\",\n",
    "        \"number_of_unique_values\": len(np.unique(values.dropna())),\n",
    "        \"number_of_missing_values\": int(values.isna().sum()),\n",
    "        \"cat_frequencies\": parse_np_matrix_to_json(np.unique(values.dropna(), return_counts=True)),\n",
    "        \"num_minimum\": None,\n",
    "        \"num_1qu\": None,\n",
    "        \"num_median\": None,\n",
    "        \"num_mean\": None,\n",
    "        \"num_3qu\": None,\n",
    "        \"num_maximum\": None\n",
    "    }\n",
    "    \n",
    "def summarize_numerical_variable(values, name, model):\n",
    "    model['preprocessing'][name] = {\n",
    "        \"name\": name,\n",
    "        \"type\": \"numerical\",\n",
    "        \"number_of_unique_values\": len(np.unique(values.dropna())),\n",
    "        \"number_of_missing_values\": int(values.isna().sum()),\n",
    "        \"cat_frequencies\": None,\n",
    "        \"num_minimum\": float(np.min(values.dropna())),\n",
    "        \"num_1qu\": float(np.percentile(values.dropna(),25)),\n",
    "        \"num_median\": float(np.percentile(values.dropna(),50)),\n",
    "        \"num_3q\": float(np.percentile(values.dropna(),75)),\n",
    "        \"num_maximum\": float(np.max(values.dropna()))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "from platform import python_version\n",
    "\n",
    "import openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import pkg_resources\n",
    "import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_by = \"Siemashko\"\n",
    "date = datetime.datetime.now().strftime(\"%d-%m-%Y\")\n",
    "dataset_id = \"openml_kc1-numeric\"\n",
    "task_type = \"regression\"\n",
    "task_target = \"NUMDEFECTS\"\n",
    "task_id = f'{task_type}_{task_target}'\n",
    "task = {\n",
    "    \"id\": task_id,\n",
    "    \"added_by\": added_by,\n",
    "    \"date\": date,\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"type\": task_type,\n",
    "    \"target\": task_target\n",
    "}\n",
    "\n",
    "with open('task.json', 'w') as fp:\n",
    "    json.dump([task], fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "datasetOpenmlId = 1070\n",
    "\n",
    "dataset = openml.datasets.get_dataset(datasetOpenmlId)\n",
    "(X, y, categorical, names) = dataset.get_data(\n",
    "    target=dataset.default_target_attribute,\n",
    "    return_categorical_indicator=True,\n",
    "    return_attribute_names=True,\n",
    "    include_ignore_attributes=True\n",
    ")\n",
    "\n",
    "vals = {}\n",
    "for i, name in enumerate(names):\n",
    "    vals[name] = X[:, i]\n",
    "vals[dataset.default_target_attribute] = y\n",
    "df = pd.DataFrame(vals)\n",
    "\n",
    "X = df.drop('NUMDEFECTS', axis=1)\n",
    "y = df.loc[:, 'NUMDEFECTS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and md5 hash - remember to remain correct order of 'categorical' variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLPRegressor'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siemashko/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/siemashko/anaconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/home/siemashko/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md5 hash: e5fd45d2ed0455daebc29e48d9f68d7f\n",
      "Directory e5fd45d2ed0455daebc29e48d9f68d7f already exists\n"
     ]
    }
   ],
   "source": [
    "transform_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train = pd.DataFrame(transform_pipeline.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "regressor = MLPRegressor(hidden_layer_sizes=[50, 15], random_state=42, max_iter=400, alpha=0.0006)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "md5 = hashlib.md5(str(regressor).encode('utf-8')).hexdigest()\n",
    "\n",
    "model = {\n",
    "    \"id\": md5,\n",
    "    \"added_by\": added_by,\n",
    "    \"date\": date,\n",
    "    \"library\": \"scikit\",\n",
    "    \"model_name\": regressor.__class__.__name__,\n",
    "    \"task_id\": task_id,\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"parameters\": regressor.get_params(),\n",
    "    \"preprocessing\": {}\n",
    "}\n",
    "\n",
    "print(f'md5 hash: {md5}')\n",
    "\n",
    "for i in range(len(names)):\n",
    "    if categorical[i]:\n",
    "        summarize_categorical_variable(X_train.loc[:,names[i]], names[i], model)\n",
    "    else:\n",
    "        summarize_numerical_variable(X_train.loc[:,names[i]], names[i], model)\n",
    "        \n",
    "try:\n",
    "    os.mkdir(f'/home/siemashko/Desktop/2019L-WarsztatyBadawcze/models/{dataset_id}/{task_id}/{md5}')\n",
    "except:\n",
    "    print(f'Directory {md5} already exists')\n",
    "    \n",
    "with open(f'/home/siemashko/Desktop/2019L-WarsztatyBadawcze/models/{dataset_id}/{task_id}/{md5}/model.json', 'w') as fp:\n",
    "    json.dump([model], fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siemashko/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(transform_pipeline.transform(X_test))\n",
    "\n",
    "audit = {\"id\": f'audit_{md5}',\n",
    "         \"date\": datetime.datetime.now().strftime(\"%d-%m-%Y\"),\n",
    "         \"added_by\": added_by,\n",
    "         \"model_id\": md5,\n",
    "         \"task_id\": task_id,\n",
    "         \"dataset_id\": dataset_id}\n",
    "\n",
    "audit['performance'] = {\n",
    "             \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "             \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "             \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "             \"R2\": r2_score(y_test, y_pred),\n",
    "         }\n",
    "\n",
    "with open(f'/home/siemashko/Desktop/2019L-WarsztatyBadawcze/models/{dataset_id}/{task_id}/{md5}/audit.json', 'w') as fp:\n",
    "    json.dump([audit], fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_py = \"\"\"#:# libraries\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "from platform import python_version\n",
    "\n",
    "import openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import pkg_resources\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "#:# config\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#:# data\n",
    "\n",
    "datasetId = 1070\n",
    "\n",
    "dataset = openml.datasets.get_dataset(datasetId)\n",
    "(X, y, categorical, names) = dataset.get_data(\n",
    "    target=dataset.default_target_attribute,\n",
    "    return_categorical_indicator=True,\n",
    "    return_attribute_names=True,\n",
    "    include_ignore_attributes=True\n",
    ")\n",
    "\n",
    "vals = {}\n",
    "for i, name in enumerate(names):\n",
    "    vals[name] = X[:, i]\n",
    "vals[dataset.default_target_attribute] = y\n",
    "df = pd.DataFrame(vals)\n",
    "\n",
    "X = df.drop('NUMDEFECTS', axis=1)\n",
    "y = df.loc[:, 'NUMDEFECTS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "#:# preprocessing\n",
    "\n",
    "transform_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train = pd.DataFrame(transform_pipeline.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "#:# model\n",
    "\n",
    "regressor = MLPRegressor(hidden_layer_sizes=[50, 15], random_state=42, max_iter=400, alpha=0.0009)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#:# hash\n",
    "#:# b5e36cb00a948148308cccec6aff6b05\n",
    "md5 = hashlib.md5(str(regressor).encode('utf-8')).hexdigest()\n",
    "print(f'md5: {md5}')\n",
    "\n",
    "#:# audit\n",
    "y_pred = regressor.predict(transform_pipeline.transform(X_test))\n",
    "\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'R2: {r2_score(y_test, y_pred)}')\n",
    "\n",
    "#:# session info\n",
    "\n",
    "# Dodaj wersję pythona w session info\n",
    "\n",
    "sessionInfo = {\n",
    "    \"python_version\": python_version(),\n",
    "    \"library_versions\":[str(d) for d in pkg_resources.working_set]\n",
    "}\n",
    "with open('sessionInfo.txt', 'w') as f:\n",
    "    json.dump(sessionInfo, f, indent=4)\n",
    "\"\"\"\n",
    "with open(f'/home/siemashko/Desktop/2019L-WarsztatyBadawcze/models/{dataset_id}/{task_id}/{md5}/code.py', 'w') as fp:\n",
    "    fp.write(code_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
